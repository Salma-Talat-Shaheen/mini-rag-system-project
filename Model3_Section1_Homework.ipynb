{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4uvozRFVHUPj5CPrh/XGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salma-Talat-Shaheen/mini-rag-system-project/blob/main/Model3_Section1_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Homework: Build and Test a Mini RAG System from Scratch üß†\n",
        "\n",
        "> **üéØ Today's Goal**: Combine the knowledge from the first three lessons (Embeddings, Retrieval, Generation) to build a functional Retrieval-Augmented Generation (RAG) system from scratch. Then, test it with a self-assessment!"
      ],
      "metadata": {
        "id": "Fg7IRyVjzb74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCUjET-gzbee",
        "outputId": "1dd3babd-ac55-449e-b8df-5fcc481f3c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Part 1: The Retriever - Finding the Right Knowledge\n",
        "\n",
        "First, we'll set up our Retriever. Its job is to take a question and find the most relevant piece of text from our knowledge base.\n",
        "\n",
        "1.  **Load the Embedding Model** (`all-MiniLM-L6-v2`)\n",
        "2.  **Create our Knowledge Base**\n",
        "3.  **Encode Everything into Embeddings**\n",
        "4.  **Calculate Similarity** to find the best match"
      ],
      "metadata": {
        "id": "99P0U4tJzjq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "# 1. Load our embedding model\n",
        "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Create a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"\n",
        "]\n",
        "\n",
        "# 3. Encode our knowledge base into embeddings\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "print(f\"‚úÖ Retriever model loaded and knowledge base encoded with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "fa46136126e443a8857ea8a865bbff61",
            "c382bd4186974a63aa8f4b3eaf9af4c6",
            "04b42aeb1a4b48aeb6eb22c23b2bc489",
            "116bfa5aea1646568d11e5ef208e8de4",
            "c7ffc905a57a43b29a10f6af73cdb907",
            "67883aad5ed84c12b9a17c017861e234",
            "2d7c82bd8ed440eda60ef9fa49bf0084",
            "854914a6e40649f6b1e550921578ee33",
            "17681fafa04947369174cbe6870aad2e",
            "8e56b18e2fc94abb85eef10e053e9585",
            "4b3cc1644ff84a1e983e5d87ac1de3e1",
            "fdb583567fa64a958e26e6b4f18ad349",
            "5b39aad97f8745c2892cec6c434dd208",
            "aebb8a16f54a420eab539a8f965a141a",
            "3fd7cbf4df944f5681adc610cf8c22ed",
            "532fe6ce954345928e189e2a307c3af9",
            "778fa508468c4ae7a16ccd02d7e63a21",
            "0a8a6c11522f4769b1b3675b3d908502",
            "6457c57c14934251940f5c42590128e2",
            "cdebaca94f444576806a5cc979f601af",
            "2931ede3e491482186d066921f12e40e",
            "004f8ba624c841c08766f3ca618dd059",
            "f60eb97d44f648cf9bd252f3773a1c90",
            "790ec353d01848a7ac95d46c1a309d0c",
            "24335c59840b4d5d814c89673d1a89dd",
            "f87ca7dc7ee4439485afd7f01822106c",
            "14e6e6712e68463ba219328a88f8b4eb",
            "a807edb4b1044c43b81c56809570477b",
            "1a52bb9f452e4f87b02f575087ad71a9",
            "e466371cd4ac469ab1cad1afcb336349",
            "65f889f112a946e9bc23b4a2795dfa29",
            "fb9869d230a440e1b5a4e9cf32610d37",
            "79189bae559a4a1886c8bdee5163d1e6",
            "7e69ab01a8e04550941bf8f0d54f688e",
            "4f5562379e5a4cbcb766be3315f1b377",
            "bf7ca37b3d29457d868fec747ac76224",
            "722c0498f9c7484692d5dad80fc00924",
            "926cdd06f2394706a15ac766b32dda9a",
            "74a6033c85b54d058ae7a7f7dd83c8d5",
            "4c0b579563fb4abb8442ae2b6cf954ff",
            "42d20123983f4243aaf5ef0688467b68",
            "7762235e6fd147d6baf988fabcd95469",
            "3b30f90f20664b86bbd29db3ca61c2a1",
            "de6574717e8d488786dbb994929456b5",
            "6ed548fc1b994727b1ff2a41bcae69a9",
            "0d384e3c21be4e2bbd5ada80f7838ad5",
            "e4c8a80b521149329870746074eb0a68",
            "b7d53e9f6b134072bec9707401f5d501",
            "82f36c3cf1414e2dab71e52b7484c53e",
            "d0060b6f43814d3fa825c13d059c8fff",
            "3ad286be41d84af496e3ae97d1d20ad0",
            "6453763a964f4487a32734cedfe43ea9",
            "d261a9f97fd24f94b0c57a35b2d9ae54",
            "c82bdbed4d674cfc97654468b8f9de60",
            "b7bb38ed43f84d4e9e30c65e27ffefb6",
            "fb64d4346cfc4323b90788505e056c49",
            "d04f5c9630394b3b8d50363cd6c80c47",
            "6e51e2d362d64b80831264f9110131f0",
            "6625b9d3e2184618ac55584c267ea475",
            "7d4bac267e3f46e0abbff8708e0dc70b",
            "76e53af456514faeb70669a1aa9d9616",
            "d02a5dbc03d24f1f85e9323f5612be33",
            "c80cbde0dccc4716a7d46c13557483f2",
            "e4dda9029a424794922d96eebf62fd69",
            "e4db32cdeeeb42a0b9ca0dd9408d0bbf",
            "d1f061b7fb69426388b900197585a147",
            "bf9a5543d80942518133b49c5171a942",
            "5287f7d8151d4404a4c82faf8c911d57",
            "52ce0c3a37c0486b9a6aaca4e4c2221e",
            "7f9c9948247c4fe1af4e552083b6e072",
            "a4f8a26f18ff454998212bd080af5d9c",
            "c6ae40dcb83340a79999f521c24a3911",
            "7635975371bb479784fb27067c29c538",
            "df0c94fd64f7400ca49c42fbe2bfda68",
            "abf97bf1ca4b4063a42f5f6844adb914",
            "e13a71aa4de9455490c0a87f066a0684",
            "0b6895e612744730b61f6aad77874124",
            "f16f4767414e482da5274dd52ffe9394",
            "43e6a8867bd44a2aa2ee50420bd961e9",
            "b6064a63657b47a0865876b788b6cabd",
            "94e6024cb040452d9924af4bd93cc503",
            "21d29b86c8d94e28a0136e3a452c6b2d",
            "e33062e53ccc4a8280e4bf17d3239e3b",
            "b6eb6dbcf3c747de8bd5793ebdd47871",
            "4947dc93cb284931afe303db1065da17",
            "9b6905f0c2884263bd277f3344596c33",
            "2371633a5d55433c901bc61a487840da",
            "722590acd90c4544ba3dff5d2aec38af",
            "5a0fe233bb4c49d480ab8d575bba94d7",
            "3f91ee781de54c4690b08490faac974a",
            "9220de1dc32a459683005a33bf477189",
            "87003dc876f94a9d8e37c0505bfc21d0",
            "6c3c7aa451494925a7d72609641ead7a",
            "c6a4550ec1304c66a8256d94cfc30154",
            "15a3f1a2f0a24fb4a65ca64aa0cad10b",
            "411a8f3908a8409cb8c309688d6d2132",
            "b4dd33e1ce6b4706aec9b2d7656aee8a",
            "7685a323ea064c86b3896e40b868da5e",
            "212a06992d084b2c93e117a90a0a4d55",
            "f615b8699af44cf9ace29dd68a675094",
            "42063fd797034560a3d3391ac3971dba",
            "e89a2101b2d84729b8e3945a96fa8d2f",
            "32c0dded994d46479cf40743406288e3",
            "e874fe5df91c4635840ab46cc17c42ed",
            "3a2e99508286494f9fb313123c3cd1a5",
            "55573bbbd70d459686f7ed75be0bb870",
            "5d77dcaac89147629d26c6ab1d95d403",
            "092e9e095dd542699669cf8693aa8a41",
            "fc22d33386564538ad5e38f6fc3ed5a4",
            "54ae89c3b37d464c965ed85157e0a29a",
            "10c5f8ea705540bca3ad4c366f61a3cc",
            "18326f92f3514aad96a9592200071ece",
            "86a44e0586a141a3afdb57765f82fe8d",
            "5416e62c0d954fa4be34e86f947d9b1a",
            "38766b0d2f614e65a4a85ce5bedec22a",
            "2342d7503bf94c188cfe47e6d9e8a7b6",
            "bdf1fe83745b4528acd1e0e12a2781d3",
            "edc4cba4a9e34f2c98fbf6037ae460e6",
            "a1d84db008ca4bf6867aa3d2a16dc01b",
            "71b6bde60fe646fbbf9178a17f6962f6",
            "c33459061af24b9ba0ce251ed17d3792"
          ]
        },
        "id": "CJHpc394zkYw",
        "outputId": "1b87a0a7-447d-44c4-93a5-3d48bf0e1a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa46136126e443a8857ea8a865bbff61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdb583567fa64a958e26e6b4f18ad349"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f60eb97d44f648cf9bd252f3773a1c90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e69ab01a8e04550941bf8f0d54f688e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ed548fc1b994727b1ff2a41bcae69a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb64d4346cfc4323b90788505e056c49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf9a5543d80942518133b49c5171a942"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f16f4767414e482da5274dd52ffe9394"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a0fe233bb4c49d480ab8d575bba94d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f615b8699af44cf9ace29dd68a675094"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10c5f8ea705540bca3ad4c366f61a3cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Retriever model loaded and knowledge base encoded with 5 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Part 2: The Generator - Extracting the Answer\n",
        "\n",
        "Now we set up our Generator. This model will take the question and the context found by the retriever and extract the exact answer from it."
      ],
      "metadata": {
        "id": "Plr7LaDsznpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our question-answering (generator) model\n",
        "generator = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "print(\"‚úÖ Generator (QA) model loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "255b8e44fe6b4f268257ba799f34c7b6",
            "f438991d29a84b17931c8cada27c45d3",
            "f871d0d0a96e40389d9cc543acc76d55",
            "b3bc0b2464b64b0f9bb97e75d3d34663",
            "57e73f70a12d40cc9a6b7a2109e72e6e",
            "55f72cb2c58c4292978b6963681d4519",
            "8e9ea9587ed94c3289f7a924941af471",
            "2cd0ab28e1754b45bea1b7d4e61b647c",
            "02872979bb464ff7957a617bfeee0ff8",
            "d6b442940876448ba3e2793abf490f2b",
            "e7eff329b8bc45fe8ab8d952a54a58a4",
            "803cf82ddc8e4d1f8255950389961876",
            "47d7db6fefc748da80abcd069ac78066",
            "44679863def048e0a1478bf213e9b28f",
            "fba57dbad0424af684f8a53a322b340c",
            "fe5393379ded48fa9c8cef1aacc87222",
            "c484b9de702947cc8df3472dbc57f7a0",
            "7313ca38ca39471f88f0884bc4c0717d",
            "e1114ea7b526404bbb24752029eb62ca",
            "3c72d0db3aa245ada7f17961d36cb42f",
            "8aa3779f9b7a408f8241f2ad504d8138",
            "2e4710b147244e01a76f87a72e1cefbb",
            "be2f5829d28642978b99935e0726e881",
            "79d4a7377ed64e7d87973dd2c4f571d9",
            "dfe1bf3da0044ae293266aed55f80f01",
            "b38cc068da7a4dd0bb50881a227703fc",
            "d3d3aa71c51c4847bb6aa45d1a239098",
            "76d1e44f8a834069b4e8353a3ae98f51",
            "82e7a92fe2814a0db1e05e8e24906510",
            "d3ba58a48faf4fad8ada3e6b51e88d63",
            "0d3c06118a29458c906562bd4d5e4ebf",
            "e195260cbddf470391485791241e895f",
            "1949690a0b624dc8aa479df1f9986e7d",
            "e9a170b729d14794b1ad167591af244d",
            "7a2f6327219c45be899aa8e820438352",
            "4469f1f9f36a4e33b15d11798562d2de",
            "134579e012524bb4a86e01bef10d5794",
            "176d3faa80924ef5a3842fbdb54fa402",
            "bfbc0c2c394b4d6eb2a35028f1772d97",
            "9043ad3a8cae4292b8167531fccbf7f9",
            "23f391b985e545d78da66463b6a8a1bf",
            "b85cbfa6dd8c409e814814e9c10414be",
            "452ab6399d504ecebfd1c01ae9f330d1",
            "00327f91a9e141b685ba96393797fe9a",
            "a4560022b5fb4decad84c13a9e23f171",
            "af9c540a6a234deea1a75e9f5a7cc495",
            "6a54a8b6f06149b294f4a96a8a868cc9",
            "5fbc2b47af1346428b861845a1cb2f49",
            "99d6e90f6a48495da26301ffa880f264",
            "bf8ba5bd3ee745d28af09b784823168c",
            "dff4801202104612b91ac4b4ba99e8f3",
            "ed3ce014e1ae421ca806429a9f7ed013",
            "3f3ea0e4298c48c3b489be4c1f289b2e",
            "cc09a2e49480461bb83892801889bc15",
            "5d460668946d419d9a79891c4f0d93ec"
          ]
        },
        "id": "sCHCTi1xzpto",
        "outputId": "89c594f0-d63f-42e6-aafd-ec5c06fe5ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "255b8e44fe6b4f268257ba799f34c7b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "803cf82ddc8e4d1f8255950389961876"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be2f5829d28642978b99935e0726e881"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9a170b729d14794b1ad167591af244d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4560022b5fb4decad84c13a9e23f171"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generator (QA) model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Part 3: Testing our RAG System\n",
        "\n",
        "Time to put it all together! The function below will simulate a full RAG pipeline and grade itself against a predefined set of questions and answers.\n",
        "\n",
        "It will test two key things:\n",
        "1.  **Retrieval Accuracy**: Did we find the right document?\n",
        "2.  **Generation Accuracy**: Did we extract the correct answer from that document?"
      ],
      "metadata": {
        "id": "kJMFmEjgzsIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_assessment():\n",
        "    \"\"\"Runs a self-assessment of the RAG pipeline with multiple questions.\"\"\"\n",
        "\n",
        "    # Define our questions, expected context keywords, and expected answers\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the highest mountain?\",\n",
        "            \"expected_keyword\": \"Everest\",\n",
        "            \"expected_answer\": \"Mount Everest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which city is home to the Louvre museum?\",\n",
        "            \"expected_keyword\": \"France\",\n",
        "            \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What process do plants use for energy?\",\n",
        "            \"expected_keyword\": \"Photosynthesis\",\n",
        "            \"expected_answer\": \"Photosynthesis\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2 # 2 points per question (1 for retrieval, 1 for generation)\n",
        "\n",
        "    print(\"--- üöÄ Starting RAG System Assessment ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "\n",
        "        # --- 1. Retrieval Step ---\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "\n",
        "        print(f\"üîé  Retrieved Context: '{retrieved_context}'\")\n",
        "\n",
        "        # Check if the retrieval was correct\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"‚úÖ  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "\n",
        "        # --- 2. Generation Step ---\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "\n",
        "        print(f\"‚úçÔ∏è  Generated Answer: '{generated_answer}'\")\n",
        "\n",
        "        # Check if the generation was correct\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"‚úÖ  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    # --- Final Score ---\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üéâüéâüéâ Perfect! Your RAG system is working as expected!\")\n",
        "    elif score >= total / 2:\n",
        "        print(\"üëç Good job! The system is mostly correct.\")\n",
        "    else:\n",
        "        print(\"üîß The system ran into some issues. Review the steps and check the logic.\")\n",
        "\n",
        "# Run the assessment!\n",
        "run_rag_assessment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvP6XETQztxR",
        "outputId": "569624bb-d4ab-4a47-edb3-055e8c41aada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üöÄ Starting RAG System Assessment ---\n",
            "\n",
            "\n",
            "--- Question 1: 'What is the highest mountain?' ---\n",
            "üîé  Retrieved Context: 'Mount Everest is the highest mountain on Earth, located in the Himalayas.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'Mount Everest'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- Question 2: 'Which city is home to the Louvre museum?' ---\n",
            "üîé  Retrieved Context: 'The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'Paris'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- Question 3: 'What process do plants use for energy?' ---\n",
            "üîé  Retrieved Context: 'Photosynthesis is the process used by plants to convert light energy into chemical energy.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'Photosynthesis'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- üèÅ Assessment Complete ---\n",
            "üéØ Final Score: 6 / 6\n",
            "üéâüéâüéâ Perfect! Your RAG system is working as expected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STUDENT TASKS üßë‚Äçüíª\n",
        "\n",
        "Now it's your turn to be the AI engineer. Your tasks are to run, analyze, and extend the RAG system you've just built."
      ],
      "metadata": {
        "id": "bmsraUmd0ruq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Execute and Understand\n",
        "\n",
        "Your first task is to simply run all the cells above and carefully read the output of the final self-assessment.\n",
        "\n",
        "* **Observe the Score:** Did the system get a perfect score (6/6)?\n",
        "* **Analyze Each Step:** For each question, look at the \"Retrieved Context\" and the \"Generated Answer.\"\n",
        "    * Did the retriever find the correct piece of knowledge?\n",
        "    * Did the generator extract the right answer from that context?"
      ],
      "metadata": {
        "id": "RT5z7ZoE0trr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 (Challenge): Add a New Question\n",
        "\n",
        "Your second task is to test the system with a new question about the **existing knowledge**.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Copy the code from the cell below. It's the same assessment function as before, but with a new test question added.\n",
        "2.  Run the cell and see if the system can answer correctly. The score should now be out of 8."
      ],
      "metadata": {
        "id": "25iQeOoo6jTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Add a new question to the assessment function\n",
        "\n",
        "def run_rag_assessment_task_2():\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the highest mountain?\",\n",
        "            \"expected_keyword\": \"Everest\",\n",
        "            \"expected_answer\": \"Mount Everest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which city is home to the Louvre museum?\",\n",
        "            \"expected_keyword\": \"France\",\n",
        "            \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What process do plants use for energy?\",\n",
        "            \"expected_keyword\": \"Photosynthesis\",\n",
        "            \"expected_answer\": \"Photosynthesis\"\n",
        "        },\n",
        "        # --- NEW QUESTION ADDED FOR TASK 2 (CORRECTED EXPECTED ANSWER) ---\n",
        "        {\n",
        "            \"question\": \"How long is the Great Wall of China?\",\n",
        "            \"expected_keyword\": \"Great Wall of China\",\n",
        "            \"expected_answer\": \"13,000 miles\" # CHANGED THIS TO BE LESS STRICT\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- üöÄ Starting RAG System Assessment (Task 2) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "        print(f\"üîé  Retrieved Context: '{retrieved_context}'\")\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"‚úÖ  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "        print(f\"‚úçÔ∏è  Generated Answer: '{generated_answer}'\")\n",
        "        # The check is \"if expected_answer is IN generated_answer\"\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"‚úÖ  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üéâüéâüéâ Perfect! Your RAG system handled the new question!\")\n",
        "    elif score >= total / 2:\n",
        "        print(\"üëç Good job! The system is mostly correct.\")\n",
        "    else:\n",
        "        print(\"üîß The system ran into some issues. Review the steps and check the logic.\")\n",
        "\n",
        "# Run the updated assessment\n",
        "run_rag_assessment_task_2()"
      ],
      "metadata": {
        "id": "6TKb4uBO6pEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127825f7-dd5d-4980-ba24-1ef2561b75ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üöÄ Starting RAG System Assessment (Task 2) ---\n",
            "\n",
            "\n",
            "--- Question 1: 'What is the highest mountain?' ---\n",
            "üîé  Retrieved Context: 'Mount Everest is the highest mountain on Earth, located in the Himalayas.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'Mount Everest'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- Question 2: 'Which city is home to the Louvre museum?' ---\n",
            "üîé  Retrieved Context: 'The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'Paris'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- Question 3: 'What process do plants use for energy?' ---\n",
            "üîé  Retrieved Context: 'Photosynthesis is the process used by plants to convert light energy into chemical energy.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'Photosynthesis'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- Question 4: 'How long is the Great Wall of China?' ---\n",
            "üîé  Retrieved Context: 'The Great Wall of China is a series of fortifications stretching over 13,000 miles.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: '13,000 miles'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- üèÅ Assessment Complete ---\n",
            "üéØ Final Score: 8 / 8\n",
            "üéâüéâüéâ Perfect! Your RAG system handled the new question!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 (Advanced Challenge): Add New Knowledge & Test It\n",
        "\n",
        "Your final and most important task is to **expand the RAG system's knowledge base** and then test it.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Add a new fact** to the `knowledge_base` in the code cell below.\n",
        "2.  **You must re-run this cell** to update the `knowledge_embeddings`! The system won't know about the new fact until you do.\n",
        "3.  Finally, run the last code cell, which has a new test question about the knowledge you just added."
      ],
      "metadata": {
        "id": "JNHQuccw7Duu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 1: Add a new sentence to the knowledge base\n",
        "\n",
        "# It's crucial to include all previous knowledge base entries,\n",
        "# plus your new one, in this list for re-encoding.\n",
        "knowledge_base_task_3 = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\",\n",
        "    # --- ADDED NEW FACT FOR TASK 3 ---\n",
        "    \"The deepest part of the world's oceans is the Mariana Trench, located in the western Pacific Ocean.\"\n",
        "]\n",
        "\n",
        "# Re-encode the updated knowledge base\n",
        "knowledge_embeddings_task_3 = retriever_model.encode(knowledge_base_task_3, convert_to_tensor=True)\n",
        "\n",
        "print(f\"‚úÖ Knowledge base updated and re-encoded with {len(knowledge_base_task_3)} documents.\")"
      ],
      "metadata": {
        "id": "4ijwpN1W7VyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441f96b7-4fb5-4385-e5a7-a441beda706d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Knowledge base updated and re-encoded with 6 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 2: Test your newly added knowledge\n",
        "\n",
        "def run_rag_assessment_task_3():\n",
        "    test_questions = [\n",
        "        # --- NEW QUESTION FOR YOUR NEW KNOWLEDGE ---\n",
        "        # Testing the new fact: \"The deepest part of the world's oceans is the Mariana Trench, located in the western Pacific Ocean.\"\n",
        "        {\n",
        "            \"question\": \"What is the deepest part of the world's oceans?\",\n",
        "            \"expected_keyword\": \"Mariana Trench\", # The keyword we expect to find in the retrieved context\n",
        "            \"expected_answer\": \"the Mariana Trench\" # The exact answer expected from the generator\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- üöÄ Starting RAG System Assessment (Task 3) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "        # Use the updated embeddings from Task 3\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        # Ensure you are using knowledge_embeddings_task_3 and knowledge_base_task_3 here!\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings_task_3)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base_task_3[top_result_index]\n",
        "        print(f\"üîé  Retrieved Context: '{retrieved_context}'\")\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"‚úÖ  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "        print(f\"‚úçÔ∏è  Generated Answer: '{generated_answer}'\")\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"‚úÖ  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üèÜüèÜüèÜ Success! You have successfully extended the knowledge of your RAG system!\")\n",
        "    # Optional: Add these if you want more nuanced feedback for partial success\n",
        "    elif score >= total / 2:\n",
        "        print(\"üëç Good job! The system is mostly correct.\")\n",
        "    else:\n",
        "        print(\"üîß The system ran into some issues. Review the steps and check the logic.\")\n",
        "\n",
        "\n",
        "# Run the final assessment\n",
        "run_rag_assessment_task_3()"
      ],
      "metadata": {
        "id": "-tgQJ13P7pCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf46de49-0d50-4fa5-d658-7900a9443e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üöÄ Starting RAG System Assessment (Task 3) ---\n",
            "\n",
            "\n",
            "--- Question 1: 'What is the deepest part of the world's oceans?' ---\n",
            "üîé  Retrieved Context: 'The deepest part of the world's oceans is the Mariana Trench, located in the western Pacific Ocean.'\n",
            "‚úÖ  Retrieval Correct!\n",
            "‚úçÔ∏è  Generated Answer: 'the Mariana Trench'\n",
            "‚úÖ  Generation Correct!\n",
            "\n",
            "--- üèÅ Assessment Complete ---\n",
            "üéØ Final Score: 2 / 2\n",
            "üèÜüèÜüèÜ Success! You have successfully extended the knowledge of your RAG system!\n"
          ]
        }
      ]
    }
  ]
}